# OpenRouter Integration - AI Gateway Implementation Guide

> **PRIORITY 1 INTEGRATION**: Transform Quest Core from single-model dependency to intelligent multi-model routing for enhanced coaching capabilities and cost optimization
> **Environment**: ✅ OPEN_ROUTER_API_KEY configured and deployed
> **Status**: Ready for immediate implementation (2-3 hours)

## 🎯 **Integration Overview**

### **Why OpenRouter for Quest Core**
OpenRouter.AI provides the perfect AI gateway solution for Quest Core's multi-coach architecture, enabling:
- **Model Flexibility**: Access to 300+ AI models through unified API
- **Cost Optimization**: Automatic routing to most cost-effective models
- **Reliability**: Fallback capabilities when providers are down
- **OpenAI Compatibility**: Drop-in replacement requiring minimal code changes
- **Coach Specialization**: Different models optimized for different coaching types

### **Immediate Business Benefits**
- **40-60% Cost Reduction**: Immediate savings on existing AI usage
- **Enhanced Coaching Quality**: Best model for each coaching type
- **System Reliability**: Multi-provider redundancy and fallback
- **Foundation Building**: Enables multi-coach architecture
- **Future-Proofing**: Easy addition of new models and providers

### **Implementation Readiness**
- ✅ **Environment**: OPEN_ROUTER_API_KEY configured
- ✅ **Dependencies**: Minimal - uses existing OpenAI package
- ✅ **Risk Level**: Low - drop-in replacement for OpenAI client
- ✅ **Time Estimate**: 2-3 hours for complete implementation
- ✅ **Immediate Testing**: Can validate cost savings immediately

## 🏗️ **Architecture Integration**

### **Current vs Future State**
```
CURRENT:
Quest Core → OpenAI API → GPT-4 (everything)

FUTURE:
Quest Core → AI Client → OpenRouter → Multiple Providers
                                    ├── OpenAI (GPT-4, GPT-3.5)
                                    ├── Anthropic (Claude-3)
                                    ├── Google (Gemini Pro)
                                    └── Meta (Llama-2)
```

### **Multi-Coach Model Mapping**
```typescript
const coachModelStrategy = {
  master: {
    model: 'openai/gpt-4-turbo',
    rationale: 'Complex orchestration requires highest reasoning capability',
    fallback: 'openai/gpt-4'
  },
  career: {
    model: 'anthropic/claude-3-sonnet', 
    rationale: 'Strategic analysis and long-term planning expertise',
    fallback: 'openai/gpt-4'
  },
  skills: {
    model: 'openai/gpt-4',
    rationale: 'Technical assessment and learning path creation',
    fallback: 'openai/gpt-3.5-turbo'
  },
  leadership: {
    model: 'google/gemini-pro',
    rationale: 'Interpersonal and management guidance',
    fallback: 'anthropic/claude-3-sonnet'
  },
  network: {
    model: 'anthropic/claude-3-sonnet',
    rationale: 'Relationship strategy and networking insights', 
    fallback: 'openai/gpt-3.5-turbo'
  }
};
```

## 🚀 **Implementation Plan**

### **Phase 1: Foundation Setup (Week 1)**

#### **1.1 Package Installation & Configuration**
```bash
# OpenRouter uses OpenAI-compatible API
npm install openai

# Environment configuration
# ✅ Already configured as OPEN_ROUTER_API_KEY in Vercel environment
echo "OPENROUTER_BASE_URL=https://openrouter.ai/api/v1" >> .env.local
```

#### **1.2 AI Client Abstraction Layer**
```typescript
// lib/ai-client.ts
import OpenAI from 'openai';

export type CoachType = 'master' | 'career' | 'skills' | 'leadership' | 'network';

export interface AIResponse {
  content: string;
  model: string;
  tokensUsed: number;
  cost: number;
  responseTime: number;
}

export class AIClient {
  private openrouter: OpenAI;
  private fallbackEnabled: boolean;
  
  constructor() {
    this.openrouter = new OpenAI({
      baseURL: process.env.OPENROUTER_BASE_URL || 'https://openrouter.ai/api/v1',
      apiKey: process.env.OPEN_ROUTER_API_KEY, // ✅ Already configured
      defaultHeaders: {
        'HTTP-Referer': process.env.NEXT_PUBLIC_APP_URL,
        'X-Title': 'Quest Core Professional Development Platform'
      }
    });
    
    this.fallbackEnabled = process.env.OPENROUTER_FALLBACK_ENABLED === 'true';
  }
  
  async generateResponse(
    coachType: CoachType,
    messages: any[],
    options: {
      temperature?: number;
      maxTokens?: number;
      stream?: boolean;
    } = {}
  ): Promise<AIResponse> {
    const startTime = Date.now();
    const model = this.selectModelForCoach(coachType);
    
    try {
      const response = await this.openrouter.chat.completions.create({
        model,
        messages,
        temperature: options.temperature ?? this.getDefaultTemperature(coachType),
        max_tokens: options.maxTokens ?? this.getDefaultMaxTokens(coachType),
        stream: options.stream ?? false
      });
      
      const tokensUsed = response.usage?.total_tokens || 0;
      
      return {
        content: response.choices[0].message.content || '',
        model,
        tokensUsed,
        cost: this.calculateCost(model, tokensUsed),
        responseTime: Date.now() - startTime
      };
    } catch (error) {
      if (this.fallbackEnabled) {
        console.warn(`Model ${model} failed, attempting fallback`);
        return this.attemptFallback(coachType, messages, options, startTime);
      }
      throw error;
    }
  }
  
  private selectModelForCoach(coachType: CoachType): string {
    const modelMap = {
      master: process.env.COACH_MODEL_MASTER || 'openai/gpt-4-turbo',
      career: process.env.COACH_MODEL_CAREER || 'anthropic/claude-3-sonnet',
      skills: process.env.COACH_MODEL_SKILLS || 'openai/gpt-4',
      leadership: process.env.COACH_MODEL_LEADERSHIP || 'google/gemini-pro',
      network: process.env.COACH_MODEL_NETWORK || 'anthropic/claude-3-sonnet'
    };
    
    return modelMap[coachType];
  }
  
  private async attemptFallback(
    coachType: CoachType,
    messages: any[],
    options: any,
    startTime: number
  ): Promise<AIResponse> {
    const fallbackModel = this.getFallbackModel(coachType);
    
    try {
      const response = await this.openrouter.chat.completions.create({
        model: fallbackModel,
        messages,
        temperature: options.temperature ?? this.getDefaultTemperature(coachType),
        max_tokens: options.maxTokens ?? this.getDefaultMaxTokens(coachType)
      });
      
      const tokensUsed = response.usage?.total_tokens || 0;
      
      return {
        content: response.choices[0].message.content || '',
        model: fallbackModel,
        tokensUsed,
        cost: this.calculateCost(fallbackModel, tokensUsed),
        responseTime: Date.now() - startTime
      };
    } catch (fallbackError) {
      throw new Error(`Both primary and fallback models failed: ${fallbackError}`);
    }
  }
  
  private getFallbackModel(coachType: CoachType): string {
    const fallbackMap = {
      master: 'openai/gpt-4',
      career: 'openai/gpt-4',
      skills: 'openai/gpt-3.5-turbo',
      leadership: 'anthropic/claude-3-sonnet',
      network: 'openai/gpt-3.5-turbo'
    };
    
    return fallbackMap[coachType];
  }
  
  private getDefaultTemperature(coachType: CoachType): number {
    const temperatureMap = {
      master: 0.7,    // Balanced for orchestration
      career: 0.8,    // Creative for strategic thinking
      skills: 0.3,    // Precise for technical assessment
      leadership: 0.9, // Creative for interpersonal guidance
      network: 0.8    // Creative for relationship strategy
    };
    
    return temperatureMap[coachType];
  }
  
  private getDefaultMaxTokens(coachType: CoachType): number {
    const tokenMap = {
      master: 1500,   // Orchestration and synthesis
      career: 1200,   // Strategic analysis
      skills: 1000,   // Technical guidance
      leadership: 1200, // Interpersonal advice
      network: 1000   // Networking strategy
    };
    
    return tokenMap[coachType];
  }
  
  private calculateCost(model: string, tokens: number): number {
    // OpenRouter pricing per 1M tokens (approximate)
    const pricingMap = {
      'openai/gpt-4-turbo': 10.0,
      'openai/gpt-4': 30.0,
      'openai/gpt-3.5-turbo': 0.5,
      'anthropic/claude-3-opus': 15.0,
      'anthropic/claude-3-sonnet': 3.0,
      'anthropic/claude-3-haiku': 0.25,
      'google/gemini-pro': 0.5
    };
    
    const costPer1M = pricingMap[model] || 1.0;
    return (tokens / 1000000) * costPer1M;
  }
}
```

### **Phase 2: Integration with Existing Systems (Week 1-2)**

#### **2.1 Voice Coaching Integration**
```typescript
// Update existing voice coaching to use AI client
// src/app/api/hume-clm-sse/chat/completions/route.ts

import { AIClient, CoachType } from '@/lib/ai-client';
import { getCoachingContext } from '@/lib/voice-session';

export async function POST(request: NextRequest) {
  const { messages, session_id, user_id } = await request.json();
  
  try {
    // Get Zep context for personalization
    const context = await getCoachingContext(user_id, messages[0].content, session_id);
    
    // Determine coach type from session metadata or default to 'master'
    const coachType: CoachType = determineCoachType(messages) || 'master';
    
    // Generate response with appropriate model
    const aiClient = new AIClient();
    const response = await aiClient.generateResponse(
      coachType,
      buildPromptWithContext(messages, context),
      { stream: true }
    );
    
    // Track costs and performance
    await trackCoachingMetrics(user_id, session_id, {
      coach_type: coachType,
      model_used: response.model,
      tokens_used: response.tokensUsed,
      cost: response.cost,
      response_time: response.responseTime
    });
    
    return new Response(response.content);
  } catch (error) {
    console.error('AI coaching error:', error);
    return NextResponse.json({ error: 'Coaching failed' }, { status: 500 });
  }
}

function determineCoachType(messages: any[]): CoachType | null {
  const lastMessage = messages[messages.length - 1]?.content?.toLowerCase() || '';
  
  if (lastMessage.includes('career') || lastMessage.includes('job')) return 'career';
  if (lastMessage.includes('skill') || lastMessage.includes('learning')) return 'skills';
  if (lastMessage.includes('leadership') || lastMessage.includes('manage')) return 'leadership';
  if (lastMessage.includes('network') || lastMessage.includes('relationship')) return 'network';
  
  return 'master'; // Default orchestrator
}
```

#### **2.2 Multi-Coach Session Management**
```typescript
// lib/multi-coach-session.ts
import { AIClient, CoachType } from '@/lib/ai-client';
import { getCoachingContext } from '@/lib/voice-session';

export interface CoachingSession {
  sessionId: string;
  userId: string;
  coaches: CoachParticipant[];
  totalCost: number;
  totalTokens: number;
  duration: number;
}

export interface CoachParticipant {
  type: CoachType;
  model: string;
  responses: string[];
  cost: number;
  tokens: number;
}

export class MultiCoachSession {
  private aiClient: AIClient;
  private session: CoachingSession;
  
  constructor(sessionId: string, userId: string) {
    this.aiClient = new AIClient();
    this.session = {
      sessionId,
      userId,
      coaches: [],
      totalCost: 0,
      totalTokens: 0,
      duration: 0
    };
  }
  
  async orchestrateCoaching(userQuery: string): Promise<string> {
    const startTime = Date.now();
    
    // 1. Master coach determines strategy
    const masterContext = await getCoachingContext(this.session.userId, userQuery);
    const masterResponse = await this.aiClient.generateResponse(
      'master',
      this.buildMasterPrompt(userQuery, masterContext),
      { temperature: 0.7 }
    );
    
    this.addCoachParticipant('master', masterResponse);
    
    // 2. Extract needed specialists from master response
    const neededCoaches = this.extractNeededCoaches(masterResponse.content);
    
    // 3. Get specialist responses in parallel
    const specialistResponses = await Promise.all(
      neededCoaches.map(async (coachType) => {
        const coachContext = await getCoachingContext(
          this.session.userId, 
          `${userQuery} - ${coachType} perspective`
        );
        
        const response = await this.aiClient.generateResponse(
          coachType,
          this.buildSpecialistPrompt(coachType, userQuery, coachContext),
          { temperature: this.getTemperatureForCoach(coachType) }
        );
        
        this.addCoachParticipant(coachType, response);
        return { coachType, response: response.content };
      })
    );
    
    // 4. Master coach synthesizes final response
    const synthesisResponse = await this.aiClient.generateResponse(
      'master',
      this.buildSynthesisPrompt(userQuery, specialistResponses),
      { temperature: 0.6 }
    );
    
    this.addCoachParticipant('master', synthesisResponse);
    
    // 5. Update session metrics
    this.session.duration = Date.now() - startTime;
    await this.saveSessionMetrics();
    
    return synthesisResponse.content;
  }
  
  private addCoachParticipant(coachType: CoachType, response: any): void {
    const existingCoach = this.session.coaches.find(c => c.type === coachType);
    
    if (existingCoach) {
      existingCoach.responses.push(response.content);
      existingCoach.cost += response.cost;
      existingCoach.tokens += response.tokensUsed;
    } else {
      this.session.coaches.push({
        type: coachType,
        model: response.model,
        responses: [response.content],
        cost: response.cost,
        tokens: response.tokensUsed
      });
    }
    
    this.session.totalCost += response.cost;
    this.session.totalTokens += response.tokensUsed;
  }
  
  private extractNeededCoaches(masterResponse: string): CoachType[] {
    const coaches: CoachType[] = [];
    
    if (masterResponse.includes('career') || masterResponse.includes('Career')) {
      coaches.push('career');
    }
    if (masterResponse.includes('skills') || masterResponse.includes('Skills')) {
      coaches.push('skills');
    }
    if (masterResponse.includes('leadership') || masterResponse.includes('Leadership')) {
      coaches.push('leadership');
    }
    if (masterResponse.includes('network') || masterResponse.includes('Network')) {
      coaches.push('network');
    }
    
    return coaches.length > 0 ? coaches : ['career']; // Default to career coach
  }
  
  private async saveSessionMetrics(): Promise<void> {
    // Save to PostgreSQL for analytics
    await prisma.coachingSession.create({
      data: {
        sessionId: this.session.sessionId,
        userId: this.session.userId,
        totalCost: this.session.totalCost,
        totalTokens: this.session.totalTokens,
        duration: this.session.duration,
        coachesUsed: this.session.coaches.map(c => c.type),
        modelsUsed: this.session.coaches.map(c => ({ coach: c.type, model: c.model })),
        createdAt: new Date()
      }
    });
  }
}
```

### **Phase 3: Cost Optimization & Performance (Week 2)**

#### **3.1 Intelligent Cost Management**
```typescript
// lib/cost-optimizer.ts
export class CostOptimizer {
  private dailyBudget: number;
  private currentSpend: number = 0;
  
  constructor(dailyBudget: number = 50) {
    this.dailyBudget = dailyBudget;
  }
  
  async selectOptimalModel(
    coachType: CoachType, 
    complexity: number,
    userTier: 'free' | 'premium' | 'enterprise' = 'free'
  ): Promise<string> {
    const remainingBudget = this.dailyBudget - this.currentSpend;
    
    // Budget-based model selection
    if (remainingBudget < 5 || userTier === 'free') {
      return this.getCostOptimizedModel(coachType);
    }
    
    // Complexity-based selection for premium users
    if (complexity > 0.8 && userTier !== 'free') {
      return this.getHighPerformanceModel(coachType);
    }
    
    return this.getBalancedModel(coachType);
  }
  
  private getCostOptimizedModel(coachType: CoachType): string {
    const costModels = {
      master: 'openai/gpt-3.5-turbo',
      career: 'anthropic/claude-3-haiku',
      skills: 'openai/gpt-3.5-turbo',
      leadership: 'openai/gpt-3.5-turbo',
      network: 'anthropic/claude-3-haiku'
    };
    
    return costModels[coachType];
  }
  
  private getHighPerformanceModel(coachType: CoachType): string {
    const highPerfModels = {
      master: 'openai/gpt-4-turbo',
      career: 'anthropic/claude-3-opus',
      skills: 'openai/gpt-4',
      leadership: 'google/gemini-pro',
      network: 'anthropic/claude-3-sonnet'
    };
    
    return highPerfModels[coachType];
  }
  
  private getBalancedModel(coachType: CoachType): string {
    const balancedModels = {
      master: 'openai/gpt-4',
      career: 'anthropic/claude-3-sonnet',
      skills: 'openai/gpt-4',
      leadership: 'google/gemini-pro',
      network: 'anthropic/claude-3-sonnet'
    };
    
    return balancedModels[coachType];
  }
}
```

#### **3.2 Performance Monitoring**
```typescript
// lib/ai-analytics.ts
export interface AIMetrics {
  sessionId: string;
  userId: string;
  coachType: CoachType;
  model: string;
  tokensUsed: number;
  cost: number;
  responseTime: number;
  userSatisfaction?: number;
  errorRate: number;
}

export class AIAnalytics {
  async trackMetrics(metrics: AIMetrics): Promise<void> {
    // Store in PostgreSQL for analysis
    await prisma.aiMetrics.create({
      data: {
        ...metrics,
        timestamp: new Date()
      }
    });
    
    // Real-time monitoring
    this.updateRealTimeStats(metrics);
  }
  
  async getModelPerformance(timeRange: string = '24h'): Promise<any> {
    const startTime = this.getStartTime(timeRange);
    
    const metrics = await prisma.aiMetrics.findMany({
      where: {
        timestamp: { gte: startTime }
      },
      select: {
        model: true,
        cost: true,
        responseTime: true,
        userSatisfaction: true,
        errorRate: true
      }
    });
    
    return this.aggregateMetrics(metrics);
  }
  
  private updateRealTimeStats(metrics: AIMetrics): void {
    // Update Redis cache for real-time dashboard
    // Send to monitoring service (e.g., DataDog, New Relic)
  }
  
  private aggregateMetrics(metrics: any[]): any {
    const grouped = metrics.reduce((acc, metric) => {
      if (!acc[metric.model]) {
        acc[metric.model] = {
          model: metric.model,
          totalCost: 0,
          avgResponseTime: 0,
          avgSatisfaction: 0,
          errorRate: 0,
          count: 0
        };
      }
      
      acc[metric.model].totalCost += metric.cost;
      acc[metric.model].avgResponseTime += metric.responseTime;
      acc[metric.model].avgSatisfaction += metric.userSatisfaction || 0;
      acc[metric.model].errorRate += metric.errorRate;
      acc[metric.model].count += 1;
      
      return acc;
    }, {});
    
    // Calculate averages
    Object.values(grouped).forEach((group: any) => {
      group.avgResponseTime /= group.count;
      group.avgSatisfaction /= group.count;
      group.errorRate /= group.count;
    });
    
    return Object.values(grouped);
  }
}
```

## 📊 **Expected Benefits & ROI**

### **Cost Savings Analysis**
```
Current (Direct OpenAI):
- GPT-4 for everything: $10-30 per 1M tokens
- Monthly estimate: $200-500

With OpenRouter Optimization:
- Master Coach (GPT-4): $10/1M tokens (20% of calls)
- Career Coach (Claude-3): $3/1M tokens (25% of calls)
- Skills Coach (GPT-4): $10/1M tokens (20% of calls)  
- Leadership Coach (Gemini Pro): $0.5/1M tokens (15% of calls)
- Network Coach (Claude-3): $3/1M tokens (20% of calls)

Expected Monthly Cost: $120-200 (40% reduction)
```

### **Performance Improvements**
- **Response Quality**: Specialized models for specialized tasks
- **Reliability**: 99.9% uptime with fallback routing
- **Latency**: 25ms overhead from OpenRouter (negligible)
- **User Experience**: Faster, more accurate coaching responses

### **Development Benefits**
- **Future-Proofing**: Easy to add new models as they become available
- **A/B Testing**: Compare model performance across different use cases
- **Scaling**: Handle increased load through intelligent routing
- **Monitoring**: Built-in analytics and cost tracking

## 🚀 **Implementation Timeline**

### **Week 1: Foundation**
- Day 1-2: Install OpenRouter, create AI client abstraction
- Day 3-4: Integrate with existing voice coaching
- Day 5-7: Test multi-coach routing

### **Week 2: Optimization**
- Day 1-2: Implement cost optimization strategies
- Day 3-4: Add performance monitoring and analytics
- Day 5-7: Testing and refinement

### **Week 3: Production**
- Day 1-2: Production deployment with feature flags
- Day 3-4: Monitor performance and costs
- Day 5-7: Full rollout and documentation

## 🚀 **Immediate Implementation Plan**

### **Next Session Priority (2-3 hours)**

#### **Phase 1: Installation & Setup (30 minutes)**
- ✅ OpenAI package already available
- ✅ OPEN_ROUTER_API_KEY already configured  
- [ ] Add OPENROUTER_BASE_URL to environment variables
- [ ] Test API connectivity with simple request

#### **Phase 2: AI Client Abstraction (60 minutes)**
- [ ] Create `lib/ai-client.ts` with OpenRouter configuration
- [ ] Implement coach-specific model routing logic  
- [ ] Add cost tracking and logging functionality
- [ ] Test model selection and fallback mechanisms

#### **Phase 3: Integration & Testing (45 minutes)**  
- [ ] Update existing voice coaching API to use new AI client
- [ ] Test coaching conversations with different models
- [ ] Validate cost reduction with real usage
- [ ] Monitor response quality across models

#### **Phase 4: Optimization & Monitoring (30 minutes)**
- [ ] Add cost tracking dashboard or logging
- [ ] Implement automatic model selection based on context
- [ ] Set up alerts for cost thresholds
- [ ] Document cost savings achieved

### **Success Metrics**
- **Cost Reduction**: Measure 40-60% savings on AI costs
- **Response Quality**: Maintain or improve coaching quality
- **System Reliability**: Fallback mechanisms working
- **Foundation Ready**: Multi-coach architecture enabled

### **Immediate Benefits**
- Start saving on AI costs from day one
- Better coaching responses through model specialization
- Foundation for advanced multi-coach features
- Improved system reliability with fallbacks

---

**OpenRouter Integration Status**: ✅ **READY FOR IMMEDIATE IMPLEMENTATION** - Environment configured, comprehensive technical specifications complete, immediate cost optimization available.